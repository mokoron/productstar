{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhxJ5G4kKTJ_"
   },
   "source": [
    "# Предварительная обработка и векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccNJ1UnLKZ9q"
   },
   "source": [
    "Сегодня мы разберем и сравним три библиотеки для предварительной обработки текста:\n",
    "\n",
    "1.   [pymorphy3](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "2.   [mystem](https://yandex.ru/dev/mystem/)\n",
    "3.   [NLTK](https://www.nltk.org/)\n",
    "\n",
    "\n",
    "**pymorphy3** – библиотека разработана для русского и украинского языков. Pymorphy умеет:\n",
    "\n",
    "1.  приводить слово к нормальной форме (например, “люди -> человек”, или “гулял -> гулять”).\n",
    "1.  ставить слово в нужную форму. Например, ставить слово во множественное число, менять падеж слова и т.д.\n",
    "1. возвращать грамматическую информацию о слове (число, род, падеж, часть речи и т.д.)\n",
    "\n",
    "При работе используется словарь OpenCorpora; для незнакомых слов строятся гипотезы. Библиотека достаточно быстрая: в настоящий момент скорость работы - от нескольких тыс слов/сек до > 100тыс слов/сек (в зависимости от выполняемой операции, интерпретатора и установленных пакетов); потребление памяти - 10…20Мб;\n",
    "Pymorphy не поддерживает стемминг\n",
    "Однако, поддерживает букву ё.\n",
    "\n",
    "\n",
    "**mystem** – вероятностный морфоанализатор для русского языка, умеет строить гипотетические разборы для множества слов — включая те слова, которых нет в словаре. Первую версию программы написали Илья Сегалович и Виталий Титов. Mystem умеет:\n",
    "1. проводить лемматизацию текста\n",
    "2. выводить набор морфологических атрибутов для каждого токена\n",
    "\n",
    "Расшифровку граммем можно посмотреть [здесь](https://yandex.ru/dev/mystem/doc/ru/grammemes-values)\n",
    "\n",
    "\n",
    "**NLTK** – Natural language Tool Kit – платформа для создания программ на Python для работы с Естественным языком. NLTK стостоит из простых в использовании интерфейсов для более чем 50 текстовых корпусов и лексических ресурсов, таких как WordNet, а также из набора библиотек обработки текста, например, классификации, токенизации, стемминга, разметки частей речи, синтаксического анализа и семантического вывода.\n",
    "[\n",
    "  Расшифровка граммем](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjhRgsdmKUdp",
    "outputId": "f5f8af85-0cb1-4bc0-837e-b4f0c7868174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pymystem3) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->pymystem3) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->pymystem3) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->pymystem3) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->pymystem3) (1.26.13)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymystem3 #для начала установим все инструменты и бибилиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geQ9QncmLE4C",
    "outputId": "8246bc88-319c-419e-d273-8c865cfa8b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pymorphy3) (0.7.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pymorphy3) (2.4.417150.4580142)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvXgATv4LFy6",
    "outputId": "2a6f4f03-4067-4181-ce3e-88807159092e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from NLTK) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from NLTK) (2024.4.16)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from NLTK) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from NLTK) (4.66.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from click->NLTK) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->NLTK) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->NLTK) (3.10.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "x11OGA-mOk_O"
   },
   "outputs": [],
   "source": [
    "# возьмем текст из лекций\n",
    "text = \"За свою карьеру я пропустил более 9000 бросков, проиграл почти 300 игр. 26 раз мне доверяли сделать финальный победный бросок, и я промахивался. Я терпел поражения снова, и снова, и снова. И именно поэтому я добился успеха.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtua80fBYBaF"
   },
   "source": [
    "## pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "nAXFdfvnPxmn"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaBFpanNPbaV",
    "outputId": "5ae6daad-1222-4614-b6eb-fad29b084ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация:\n",
      "за свой карьера я пропускать более 9000 бросок, проигрывать почти 300 игра. 26 раз я доверять сделать финальный победный бросок, и я промахиваться. я терпеть поражение снова, и снова, и снова. и именно поэтому я добиваться успех.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создаем экземпляр Mystem, который будет использоваться для выполнения морфологического анализа текста\n",
    "mystem = Mystem()\n",
    "\n",
    "# Лемматизация. Метод lemmatize() выполняет лемматизацию текста и возвращает список лемм (начальных форм слов).\n",
    "lemmas_mystem = mystem.lemmatize(text)\n",
    "print(\"Лемматизация:\")\n",
    "print(\"\".join(lemmas_mystem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9t2f4zxfWZl7",
    "outputId": "af056570-f9e7-4c79-dfd5-aa1c65d43ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'за', 'wt': 1, 'gr': 'PR='}], 'text': 'За'}, {'text': ' '}, {'analysis': [{'lex': 'свой', 'wt': 1, 'gr': 'APRO=вин,ед,жен'}], 'text': 'свою'}, {'text': ' '}, {'analysis': [{'lex': 'карьера', 'wt': 0.9589808846, 'gr': 'S,жен,неод=вин,ед'}], 'text': 'карьеру'}, {'text': ' '}, {'analysis': [{'lex': 'я', 'wt': 0.9999716281, 'gr': 'SPRO,ед,1-л=им'}], 'text': 'я'}, {'text': ' '}, {'analysis': [{'lex': 'пропускать', 'wt': 0.7310513447, 'gr': 'V=прош,ед,изъяв,муж,сов'}], 'text': 'пропустил'}, {'text': ' '}, {'analysis': [{'lex': 'более', 'wt': 0.9999468251, 'gr': 'ADV='}], 'text': 'более'}, {'text': ' '}, {'text': '9000'}, {'text': ' '}, {'analysis': [{'lex': 'бросок', 'wt': 1, 'gr': 'S,муж,неод=род,мн'}], 'text': 'бросков'}, {'text': ', '}, {'analysis': [{'lex': 'проигрывать', 'wt': 1, 'gr': 'V,пе=прош,ед,изъяв,муж,сов'}], 'text': 'проиграл'}, {'text': ' '}, {'analysis': [{'lex': 'почти', 'wt': 0.9984513216, 'gr': 'ADV='}], 'text': 'почти'}, {'text': ' '}, {'text': '300'}, {'text': ' '}, {'analysis': [{'lex': 'игра', 'wt': 1, 'gr': 'S,жен,неод=род,мн'}], 'text': 'игр'}, {'text': '. '}, {'text': '26'}, {'text': ' '}, {'analysis': [{'lex': 'раз', 'wt': 0.9278416023, 'gr': 'S,муж,неод=(вин,ед|род,мн|им,ед)'}], 'text': 'раз'}, {'text': ' '}, {'analysis': [{'lex': 'я', 'wt': 1, 'gr': 'SPRO,ед,1-л=(пр|дат)'}], 'text': 'мне'}, {'text': ' '}, {'analysis': [{'lex': 'доверять', 'wt': 1, 'gr': 'V,пе=прош,мн,изъяв,несов'}], 'text': 'доверяли'}, {'text': ' '}, {'analysis': [{'lex': 'сделать', 'wt': 1, 'gr': 'V,сов,пе=инф'}], 'text': 'сделать'}, {'text': ' '}, {'analysis': [{'lex': 'финальный', 'wt': 1, 'gr': 'A=(вин,ед,полн,муж,неод|им,ед,полн,муж)'}], 'text': 'финальный'}, {'text': ' '}, {'analysis': [{'lex': 'победный', 'wt': 1, 'gr': 'A=(вин,ед,полн,муж,неод|им,ед,полн,муж)'}], 'text': 'победный'}, {'text': ' '}, {'analysis': [{'lex': 'бросок', 'wt': 0.9756054758, 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'бросок'}, {'text': ', '}, {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'я', 'wt': 0.9999716281, 'gr': 'SPRO,ед,1-л=им'}], 'text': 'я'}, {'text': ' '}, {'analysis': [{'lex': 'промахиваться', 'wt': 1, 'gr': 'V,нп=прош,ед,изъяв,муж,несов'}], 'text': 'промахивался'}, {'text': '. '}, {'analysis': [{'lex': 'я', 'wt': 0.9999716281, 'gr': 'SPRO,ед,1-л=им'}], 'text': 'Я'}, {'text': ' '}, {'analysis': [{'lex': 'терпеть', 'wt': 1, 'gr': 'V,несов,пе=прош,ед,изъяв,муж'}], 'text': 'терпел'}, {'text': ' '}, {'analysis': [{'lex': 'поражение', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'поражения'}, {'text': ' '}, {'analysis': [{'lex': 'снова', 'wt': 1, 'gr': 'ADV='}], 'text': 'снова'}, {'text': ', '}, {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'снова', 'wt': 1, 'gr': 'ADV='}], 'text': 'снова'}, {'text': ', '}, {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'снова', 'wt': 1, 'gr': 'ADV='}], 'text': 'снова'}, {'text': '. '}, {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'И'}, {'text': ' '}, {'analysis': [{'lex': 'именно', 'wt': 1, 'gr': 'PART='}], 'text': 'именно'}, {'text': ' '}, {'analysis': [{'lex': 'поэтому', 'wt': 1, 'gr': 'ADVPRO='}], 'text': 'поэтому'}, {'text': ' '}, {'analysis': [{'lex': 'я', 'wt': 0.9999716281, 'gr': 'SPRO,ед,1-л=им'}], 'text': 'я'}, {'text': ' '}, {'analysis': [{'lex': 'добиваться', 'wt': 1, 'gr': 'V,нп=прош,ед,изъяв,муж,сов'}], 'text': 'добился'}, {'text': ' '}, {'analysis': [{'lex': 'успех', 'wt': 1, 'gr': 'S,муж,неод=род,ед'}], 'text': 'успеха'}, {'text': '.'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Метод analyze() анализирует текст и возвращает результаты в виде списка объектов, представляющих анализ каждого слова в тексте.\n",
    "analyser = mystem.analyze(text)\n",
    "print (analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zny0nblkWgOU",
    "outputId": "8ce5e423-74f0-4ad7-993f-b646b8d3de95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Стемминг:\n",
      "За свою карьеру я пропустил более бросков проиграл почти игр раз мне доверяли сделать финальный победный бросок и я промахивался Я терпел поражения снова и снова и снова И именно поэтому я добился успеха\n"
     ]
    }
   ],
   "source": [
    "# Стемминг.\n",
    "stemmed_words = [analysis.get('text') for analysis in analyser if 'analysis' in analysis]\n",
    "print(\"\\nСтемминг:\")\n",
    "print(\" \".join(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVLy4M-lYVTe"
   },
   "source": [
    "Давайте внимательнее посмотрим на результат стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1lMoPD7WK7E",
    "outputId": "b3a0c989-a8a8-42af-8ddc-4883f6272b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS-разметка:\n",
      "За-PR=\n",
      " \n",
      "свою-APRO=вин,ед,жен\n",
      " \n",
      "карьеру-S,жен,неод=вин,ед\n",
      " \n",
      "я-SPRO,ед,1-л=им\n",
      " \n",
      "пропустил-V=прош,ед,изъяв,муж,сов\n",
      " \n",
      "более-ADV=\n",
      " \n",
      "9000\n",
      " \n",
      "бросков-S,муж,неод=род,мн\n",
      ", \n",
      "проиграл-V,пе=прош,ед,изъяв,муж,сов\n",
      " \n",
      "почти-ADV=\n",
      " \n",
      "300\n",
      " \n",
      "игр-S,жен,неод=род,мн\n",
      ". \n",
      "26\n",
      " \n",
      "раз-S,муж,неод=(вин,ед|род,мн|им,ед)\n",
      " \n",
      "мне-SPRO,ед,1-л=(пр|дат)\n",
      " \n",
      "доверяли-V,пе=прош,мн,изъяв,несов\n",
      " \n",
      "сделать-V,сов,пе=инф\n",
      " \n",
      "финальный-A=(вин,ед,полн,муж,неод|им,ед,полн,муж)\n",
      " \n",
      "победный-A=(вин,ед,полн,муж,неод|им,ед,полн,муж)\n",
      " \n",
      "бросок-S,муж,неод=(вин,ед|им,ед)\n",
      ", \n",
      "и-CONJ=\n",
      " \n",
      "я-SPRO,ед,1-л=им\n",
      " \n",
      "промахивался-V,нп=прош,ед,изъяв,муж,несов\n",
      ". \n",
      "Я-SPRO,ед,1-л=им\n",
      " \n",
      "терпел-V,несов,пе=прош,ед,изъяв,муж\n",
      " \n",
      "поражения-S,сред,неод=(вин,мн|род,ед|им,мн)\n",
      " \n",
      "снова-ADV=\n",
      ", \n",
      "и-CONJ=\n",
      " \n",
      "снова-ADV=\n",
      ", \n",
      "и-CONJ=\n",
      " \n",
      "снова-ADV=\n",
      ". \n",
      "И-CONJ=\n",
      " \n",
      "именно-PART=\n",
      " \n",
      "поэтому-ADVPRO=\n",
      " \n",
      "я-SPRO,ед,1-л=им\n",
      " \n",
      "добился-V,нп=прош,ед,изъяв,муж,сов\n",
      " \n",
      "успеха-S,муж,неод=род,ед\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POS-разметка\n",
    "pos = mystem.analyze(text)\n",
    "print(\"\\nPOS-разметка:\")\n",
    "for analysis in pos:\n",
    "    if 'analysis' in analysis and analysis['analysis']:\n",
    "        print(f\"{analysis['text']}-{analysis['analysis'][0]['gr']}\")  #Если объект анализа содержит информацию о морфологическом анализе,\n",
    "                                                                        #то эта строка выводит текст слова и его частеречную разметку в формате\n",
    "                                                                        #\"слово - часть речи и грамматические характеристики\".\n",
    "    else:\n",
    "        print(analysis['text']) # в противном случае выводим исходный текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRjIGyvQYFo-"
   },
   "source": [
    "## pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "XMNo1r4VP7I8"
   },
   "outputs": [],
   "source": [
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JJ9QH58Tqq_",
    "outputId": "76cc4fbc-94fc-4607-b449-a4402e535f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация:\n",
      "за свой карьера я пропустить более 9000 бросков, проиграть почти 300 игр. 26 раз я доверять сделать финальный победный бросок, и я промахивался. я терпеть поражение снова, и снова, и снова. и именно поэтому я добиться успеха.\n"
     ]
    }
   ],
   "source": [
    "# Создаем экземпляр pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "# Лемматизация\n",
    "lemmas_morph = [morph.parse(word)[0].normal_form for word in text.split()]\n",
    "print(\"Лемматизация:\")\n",
    "print(\" \".join(lemmas_morph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стемминг (pymorphy3 не поддерживает стемминг, только лемматизацию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPf4j4joTv3_",
    "outputId": "034b30d5-bab5-4756-9ec2-b326257be8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS-разметка:\n",
      "За - PREP\n",
      "свою - ADJF\n",
      "карьеру - NOUN\n",
      "я - NPRO\n",
      "пропустил - VERB\n",
      "более - ADVB\n",
      "9000 - None\n",
      "бросков, - None\n",
      "проиграл - VERB\n",
      "почти - ADVB\n",
      "300 - None\n",
      "игр. - None\n",
      "26 - None\n",
      "раз - NOUN\n",
      "мне - NPRO\n",
      "доверяли - VERB\n",
      "сделать - INFN\n",
      "финальный - ADJF\n",
      "победный - ADJF\n",
      "бросок, - None\n",
      "и - CONJ\n",
      "я - NPRO\n",
      "промахивался. - None\n",
      "Я - NPRO\n",
      "терпел - VERB\n",
      "поражения - NOUN\n",
      "снова, - None\n",
      "и - CONJ\n",
      "снова, - None\n",
      "и - CONJ\n",
      "снова. - None\n",
      "И - CONJ\n",
      "именно - PRCL\n",
      "поэтому - ADVB\n",
      "я - NPRO\n",
      "добился - VERB\n",
      "успеха. - None\n"
     ]
    }
   ],
   "source": [
    "# POS-разметка\n",
    "pos_tags = [morph.parse(word)[0].tag.POS for word in text.split()]\n",
    "print(\"\\nPOS-разметка:\")\n",
    "for word, pos in zip(text.split(), pos_tags):\n",
    "    print(f\"{word} - {pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xBY6cokklQh"
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E3PTM-RYMak",
    "outputId": "2ad1a7c1-9e88-4c9d-a635-9a56dfece783"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuliya_ru/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yuliya_ru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading ruscorpora: Package 'ruscorpora' not found\n",
      "[nltk_data]     in index\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yuliya_ru/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /Users/yuliya_ru/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuliya_ru/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка необходимых ресурсов NLTK\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm-FFFb4k08W",
    "outputId": "ca206069-42a5-428b-ac41-60389a0a9178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лемматизация:\n",
      "I 've missed more than 9,000 shot in my career . I 've lost almost 300 game . Twenty-six time , I 've been trusted to take the game-winning shot and missed . I 've failed over and over and over again in my life . And that is why I succeed .\n"
     ]
    }
   ],
   "source": [
    "# Токенизация текста\n",
    "tokens = word_tokenize(\"I've missed more than 9,000 shots in my career. I've lost almost 300 games. Twenty-six times, I've been trusted to take the game-winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\")\n",
    "\n",
    "# Лемматизация\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas_nltk = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Лемматизация:\")\n",
    "print(\" \".join(lemmas_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fhrH2hwk7Pa",
    "outputId": "8f0dd202-bf7a-4a75-ff8c-648cd1fb7f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Стемминг:\n",
      "за сво карьер я пропуст бол 9000 броск , проигра почт 300 игр . 26 раз мне доверя сдела финальн победн бросок , и я промахива . я терпел поражен снов , и снов , и снов . и имен поэт я доб успех .\n"
     ]
    }
   ],
   "source": [
    "# Стемминг\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"\\nСтемминг:\")\n",
    "print(\" \".join(stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJQmrVXGk-SO",
    "outputId": "375caa5e-5ee4-47bb-8fd6-07090169eeb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS-разметка:\n",
      "За-JJ\n",
      "свою-NNP\n",
      "карьеру-NNP\n",
      "я-NNP\n",
      "пропустил-NNP\n",
      "более-NNP\n",
      "9000-CD\n",
      "бросков-NNP\n",
      ",-,\n",
      "проиграл-NNP\n",
      "почти-VBZ\n",
      "300-CD\n",
      "игр-NN\n",
      ".-.\n",
      "26-CD\n",
      "раз-JJ\n",
      "мне-NNP\n",
      "доверяли-NNP\n",
      "сделать-NNP\n",
      "финальный-NNP\n",
      "победный-NNP\n",
      "бросок-NNP\n",
      ",-,\n",
      "и-NNP\n",
      "я-NNP\n",
      "промахивался-NNP\n",
      ".-.\n",
      "Я-VB\n",
      "терпел-JJ\n",
      "поражения-NNP\n",
      "снова-NNP\n",
      ",-,\n",
      "и-NNP\n",
      "снова-NNP\n",
      ",-,\n",
      "и-NNP\n",
      "снова-NNP\n",
      ".-.\n",
      "И-VB\n",
      "именно-JJ\n",
      "поэтому-NNP\n",
      "я-NNP\n",
      "добился-NNP\n",
      "успеха-NNP\n",
      ".-.\n"
     ]
    }
   ],
   "source": [
    "# POS-разметка\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS-разметка:\")\n",
    "for token, pos_tag in pos_tags:\n",
    "    print(f\"{token}-{pos_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS-разметка для русского языка:\n",
      "За-PR\n",
      "свою-A-PRO=f\n",
      "карьеру-S\n",
      "я-S-PRO\n",
      "пропустил-V\n",
      "более-NUM=comp\n",
      "9000-NUM=ciph\n",
      "бросков-S\n",
      ",-NONLEX\n",
      "проиграл-V\n",
      "почти-ADV\n",
      "300-NUM=ciph\n",
      "игр-S\n",
      ".-NONLEX\n",
      "26-NUM=ciph\n",
      "раз-S\n",
      "мне-S-PRO\n",
      "доверяли-V\n",
      "сделать-V\n",
      "финальный-A=m\n",
      "победный-A=m\n",
      "бросок-S\n",
      ",-NONLEX\n",
      "и-CONJ\n",
      "я-S-PRO\n",
      "промахивался-V\n",
      ".-NONLEX\n",
      "Я-S-PRO\n",
      "терпел-V\n",
      "поражения-S\n",
      "снова-ADV\n",
      ",-NONLEX\n",
      "и-CONJ\n",
      "снова-ADV\n",
      ",-NONLEX\n",
      "и-CONJ\n",
      "снова-ADV\n",
      ".-NONLEX\n",
      "И-CONJ\n",
      "именно-PART\n",
      "поэтому-ADV-PRO\n",
      "я-S-PRO\n",
      "добился-V\n",
      "успеха-S\n",
      ".-NONLEX\n"
     ]
    }
   ],
   "source": [
    "# Выведем частеречную разметку для русского языка\n",
    "pos_ru=pos_tag(tokens, lang='rus') \n",
    "\n",
    "print(\"\\nPOS-разметка для русского языка:\")\n",
    "for token, pos_tag in pos_ru:\n",
    "    print(f\"{token}-{pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUPCsYTTm9jO"
   },
   "source": [
    "Давайте сравним результаты лемматизации трех библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9XBftQAlA8A",
    "outputId": "861d60b5-f81c-4e61-ef47-3b111a5d8d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['за', ' ', 'свой', ' ', 'карьера', ' ', 'я', ' ', 'пропускать', ' ', 'более', ' ', '9000', ' ', 'бросок', ', ', 'проигрывать', ' ', 'почти', ' ', '300', ' ', 'игра', '. ', '26', ' ', 'раз', ' ', 'я', ' ', 'доверять', ' ', 'сделать', ' ', 'финальный', ' ', 'победный', ' ', 'бросок', ', ', 'и', ' ', 'я', ' ', 'промахиваться', '. ', 'я', ' ', 'терпеть', ' ', 'поражение', ' ', 'снова', ', ', 'и', ' ', 'снова', ', ', 'и', ' ', 'снова', '. ', 'и', ' ', 'именно', ' ', 'поэтому', ' ', 'я', ' ', 'добиваться', ' ', 'успех', '.', '\\n']\n",
      "['за', 'свой', 'карьера', 'я', 'пропустить', 'более', '9000', 'бросков,', 'проиграть', 'почти', '300', 'игр.', '26', 'раз', 'я', 'доверять', 'сделать', 'финальный', 'победный', 'бросок,', 'и', 'я', 'промахивался.', 'я', 'терпеть', 'поражение', 'снова,', 'и', 'снова,', 'и', 'снова.', 'и', 'именно', 'поэтому', 'я', 'добиться', 'успеха.']\n"
     ]
    }
   ],
   "source": [
    "print (lemmas_mystem)\n",
    "print (lemmas_morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGoqES_UoHZx"
   },
   "source": [
    "Mystem отделяет пробелы в отдельные леммы, pymorphy склеивает знак препинания со словом 'бросков,', а NLTK сохраняет регистр слов. Так происходит из-за разницы токенизации в различных библиотеках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "LtMj1pDVn7BI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "self_stopWords = nltk_stopwords.words(\"russian\")\n",
    "print(self_stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRfcQq3XrBsv"
   },
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m13eAgPPrLUS"
   },
   "source": [
    "## 1. Мешок слов\n",
    "\n",
    "\n",
    "1. Токенизация\n",
    "входной текст разбивается на токены\n",
    "\n",
    "2. Создание словаря\n",
    "выбираем уникальные слова и сортируем в алфавитном порядке\n",
    "\n",
    "3. Создание вектора\n",
    "Из получившегося словаря создается разреженная матрица. В этой разреженной матрице каждая строка представляет вектор предложения, длина которого равна размеру словаря.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "S3JRnsm2rDmi"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "5Q2hYOGRrqkP"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer() # Сделаем экземпляр объекта CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n30zpTkUsCib"
   },
   "source": [
    "В качестве \"коллекции текстов\" возьмем пример с лекции – детскую песенку, где каждая строка будет расцениваться как отдельный документ.\n",
    "В лекции мы получили следующие вектора:\n",
    "\n",
    "[1 1 1 0 0 0 0 0]\n",
    "\n",
    "[1 1 0 1 0 0 0 0]\n",
    "\n",
    "[1 1 0 0 1 1 1 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "Krs1dU3lwhvu"
   },
   "outputs": [],
   "source": [
    "text_collecition = ['Какой чудесный день',\n",
    "   'Какой чудесный пень',\n",
    "   'Какой чудесный я и песенка моя',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcmMztrBr1c_",
    "outputId": "4d09a6f0-3bf0-4543-8277-9e2bf01997ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 1]\n",
      " [0 1 0 1 0 1]\n",
      " [0 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = cv.fit_transform(text_collecition) # Преобразуем тексты в матрицу признаков или векторизуем нашу коллекцию\n",
    "X = X.toarray()   #преобразуем вектора в массив NumPy для просмотра\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3We9IqVypYz"
   },
   "source": [
    "Вектора не совпадают, мы получили более короткие векторв! Давайте выведем словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUzJgpLpr4U5",
    "outputId": "486520ff-dc76-4743-fb73-f09a3b4adf06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['день', 'какой', 'моя', 'пень', 'песенка', 'чудесный']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX0WBevSyx9x"
   },
   "source": [
    "Видим, что слова \"я\" и \"и\" не вошли в словарь для построения векторов. CountVectorizer игнорирует односимвольные слова, чтобы учесть абсолютно все слова в нашем векторе, добавим параметр token_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eozVSrcyE2G",
    "outputId": "e98e6319-6bef-4d3a-e595-2891215f60c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 1 0]\n",
      " [0 1 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_single = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_single = vectorizer_single.fit_transform(text_collecition)\n",
    "\n",
    "print(X_single.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHeSxILqyO9z",
    "outputId": "c3bffeaa-96d5-4b98-817d-d9d62810cbab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['день', 'и', 'какой', 'моя', 'пень', 'песенка', 'чудесный', 'я']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer_single.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTmHlUzn0RYl"
   },
   "source": [
    "Получили векторное представление песенки, где каждая строка – равна вектору. Количество координат каждого вектора равно количеству уникальных слов во всей песенке (во всей коллекции текстов) или длине словаря.\n",
    "\n",
    "Мы получили униграммное представление, а что, если посмотреть попарно встречающиеся слова – то есть биграммы? Это можно сделать изменив аргумент ngram_range при создании объекта CountVectorizer.\n",
    "\n",
    "это представление можно перенастроить и для n-граммных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "4uAq6_y40BaQ"
   },
   "outputs": [],
   "source": [
    "cv_2 = CountVectorizer(ngram_range=(2,2))\n",
    "X_2 = cv_2.fit_transform(text_collecition)\n",
    "X_2 = X_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzuEtt6P1IjE",
    "outputId": "fe1ae53b-f0e6-4786-9fb7-ad2ea9e0a704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Векторное представление биграмм:\n",
      "[[1 0 1 0 0]\n",
      " [1 0 0 1 0]\n",
      " [1 1 0 0 1]]\n",
      "\n",
      "Словарь биграмм:\n",
      "['какой чудесный', 'песенка моя', 'чудесный день', 'чудесный пень', 'чудесный песенка']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nВекторное представление биграмм:\")\n",
    "print (X_2)\n",
    "print(\"\\nСловарь биграмм:\")\n",
    "print (sorted(cv_2.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdsov54d2BQN"
   },
   "source": [
    "## 2. TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) — это статистическая мера, используемая в обработке текста для оценки важности слова в контексте документа или корпуса документов\n",
    "\n",
    "TF-IDF учитывает значимость слова — чем выше показатель, тем важнее слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "8_qOBMAW1byU"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "_mlTK8B23bKk"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() #Создадим экземпляр объекта TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "yd2Mm8JX3fdh"
   },
   "outputs": [],
   "source": [
    "transformed = tfidf.fit_transform(text_collecition) #преобразуем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxkJjtwE3rS3"
   },
   "source": [
    "Посмотрим, какие признаки важны, а какие имеют меньший вес и меньший вклад."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "ew3wt12q3lUB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Vf4IFkY3r4d",
    "outputId": "2ba0beda-bbb3-4901-96a3-56859789fd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TF-IDF\n",
      "день      0.767495\n",
      "какой     0.453295\n",
      "чудесный  0.453295\n",
      "моя       0.000000\n",
      "пень      0.000000\n",
      "песенка   0.000000\n"
     ]
    }
   ],
   "source": [
    "# Создаем таблицу данных с названиями признаков, то есть со словами из коллекции текстов\n",
    "df = pd.DataFrame(transformed[0].T.todense(), # При векторизации мы получили разреженную матрицу,\n",
    "                                              # в коротой очень много нулей и не так много действительных значений,\n",
    "                                              # такую матрицу удобнее представить в виде Numpy массива с помощью метода метод todense()\n",
    "  index=tfidf.get_feature_names_out(), columns=[\"TF-IDF\"]) # Полный словарь токенизированных слов получаем с помощью функции get_feature_names_out()\n",
    "df = df.sort_values('TF-IDF', ascending=False)   # сортируем значения от наибольшего значения к наименьшему\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tS4Z_wp4ZDl"
   },
   "source": [
    "Согласно TF-IDF, слово \"день\" — самый важный признак, для модели также важны \"какой\" и \"чудесный\", а другие слова, которые использовались для создания признаков в подходе «мешок слов», равны нулю.\n",
    "\n",
    "В подходе TF-IDF тоже можно применять биграммы, триграммы и N-граммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0cwccSD6pON"
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "Перейдем от методов, основанных на частотности слов к эмбеддингами. Посмотрим, как на практике использовать word2vec для создания эмбеддингов.\n",
    "\n",
    "У нас есть два варинта: воспользоваться предобученной моделью или обучить модель самостоятельно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI4mt9-f5-c9"
   },
   "source": [
    "Воспользуемся предварительно обученной моделью Google. Скачать модель можно по [этой ссылке ](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g)\n",
    "\n",
    " Эту модель вы можете забрать отсюда и ниже указать путь к разархивированному файлу, или получить её с помощью следующих команд Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "im5IXMU73vz1",
    "outputId": "0a624f55-0efa-48f7-b90d-43509e359fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CavAZPZo-okn",
    "outputId": "5bea13e7-c5f4-4aa7-dd2f-1e238a8d2527"
   },
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "2J9ERIt1AJLC",
    "outputId": "bc16ea52-b7fb-46c8-a65f-79b5cdd63abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово apple найдено в модели.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Путь к файлу предобученной модели Word2Vec\n",
    "path_to_model = 'productstar/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Загрузка модели\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "\n",
    "# Проверка наличия слов в словаре\n",
    "word_to_check='apple'\n",
    "\n",
    "if word_to_check in word2vec_model:\n",
    "    print(\"Слово\", word_to_check, \"найдено в модели.\")\n",
    "else:\n",
    "    print(\"Слово\", word_to_check, \"отсутствует в модели.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "x_9wQaQzBVtc"
   },
   "outputs": [],
   "source": [
    "# Выведем вектор нашего слова, мы получили 300 мерный вектор\n",
    "vect = word2vec_model['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "uKFcwhQVAu6T",
    "outputId": "69b4d33d-8b4f-4e0d-a966-965fd02bd372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219  0.16210938\n",
      "  0.3046875  -0.1796875  -0.12109375  0.25390625 -0.01428223 -0.06396484\n",
      " -0.08056641 -0.05688477 -0.19628906  0.2890625  -0.05151367  0.14257812\n",
      " -0.10498047 -0.04736328 -0.34765625  0.35742188  0.265625    0.00188446\n",
      " -0.01586914  0.00195312 -0.35546875  0.22167969  0.05761719  0.15917969\n",
      "  0.08691406 -0.0267334  -0.04785156  0.23925781 -0.05981445  0.0378418\n",
      "  0.17382812 -0.41796875  0.2890625   0.32617188  0.02429199 -0.01647949\n",
      " -0.06494141 -0.08886719  0.07666016 -0.15136719  0.05249023 -0.04199219\n",
      " -0.05419922  0.00108337 -0.20117188  0.12304688  0.09228516  0.10449219\n",
      " -0.00408936 -0.04199219  0.01409912 -0.02111816 -0.13476562 -0.24316406\n",
      "  0.16015625 -0.06689453 -0.08984375 -0.07177734 -0.00595093 -0.00482178\n",
      " -0.00089264 -0.30664062 -0.0625      0.07958984 -0.00909424 -0.04492188\n",
      "  0.09960938 -0.33398438 -0.3984375   0.05541992 -0.06689453 -0.04467773\n",
      "  0.11767578 -0.13964844 -0.26367188  0.17480469 -0.17382812 -0.40625\n",
      " -0.06738281 -0.07617188  0.09423828  0.20996094 -0.16308594 -0.08691406\n",
      " -0.0534668  -0.10351562 -0.07617188 -0.11083984 -0.03515625 -0.14941406\n",
      "  0.0378418   0.38671875  0.14160156 -0.2890625  -0.16894531 -0.140625\n",
      " -0.04174805  0.22753906  0.24023438 -0.01599121 -0.06787109  0.21875\n",
      " -0.42382812 -0.5625     -0.49414062 -0.3359375   0.13378906  0.01141357\n",
      "  0.13671875  0.0324707   0.06835938 -0.27539062 -0.15917969  0.00121307\n",
      "  0.01208496 -0.0039978   0.00442505 -0.04541016  0.08642578  0.09960938\n",
      " -0.04296875 -0.11328125  0.13867188  0.41796875 -0.28320312 -0.07373047\n",
      " -0.11425781  0.08691406 -0.02148438  0.328125   -0.07373047 -0.01348877\n",
      "  0.17773438 -0.02624512  0.13378906 -0.11132812 -0.12792969 -0.12792969\n",
      "  0.18945312 -0.13867188  0.29882812 -0.07714844 -0.37695312 -0.10351562\n",
      "  0.16992188 -0.10742188 -0.29882812  0.00866699 -0.27734375 -0.20996094\n",
      " -0.1796875  -0.19628906 -0.22167969  0.08886719 -0.27734375 -0.13964844\n",
      "  0.15917969  0.03637695  0.03320312 -0.08105469  0.25390625 -0.08691406\n",
      " -0.21289062 -0.18945312 -0.22363281  0.06542969 -0.16601562  0.08837891\n",
      " -0.359375   -0.09863281  0.35546875 -0.00741577  0.19042969  0.16992188\n",
      " -0.06005859 -0.20605469  0.08105469  0.12988281 -0.01135254  0.33203125\n",
      " -0.08691406  0.27539062 -0.03271484  0.12011719 -0.0625      0.1953125\n",
      " -0.10986328 -0.11767578  0.20996094  0.19921875  0.02954102 -0.16015625\n",
      "  0.00276184 -0.01367188  0.03442383 -0.19335938  0.00352478 -0.06542969\n",
      " -0.05566406  0.09423828  0.29296875  0.04052734 -0.09326172 -0.10107422\n",
      " -0.27539062  0.04394531 -0.07275391  0.13867188  0.02380371  0.13085938\n",
      "  0.00236511 -0.2265625   0.34765625  0.13574219  0.05224609  0.18164062\n",
      "  0.0402832   0.23730469 -0.16992188  0.10058594  0.03833008  0.10839844\n",
      " -0.05615234 -0.00946045  0.14550781 -0.30078125 -0.32226562  0.18847656\n",
      " -0.40234375 -0.3125     -0.08007812 -0.26757812  0.16699219  0.07324219\n",
      "  0.06347656  0.06591797  0.17285156 -0.17773438  0.00276184 -0.05761719\n",
      " -0.2265625  -0.19628906  0.09667969  0.13769531 -0.49414062 -0.27929688\n",
      "  0.12304688 -0.30078125  0.01293945 -0.1875     -0.20898438 -0.1796875\n",
      " -0.16015625 -0.03295898  0.00976562  0.25390625 -0.25195312  0.00210571\n",
      "  0.04296875  0.01184082 -0.20605469  0.24804688 -0.203125   -0.17773438\n",
      "  0.07275391  0.04541016  0.21679688 -0.2109375   0.14550781 -0.16210938\n",
      "  0.20410156 -0.19628906 -0.35742188  0.35742188 -0.11962891  0.35742188\n",
      "  0.10351562  0.07080078 -0.24707031 -0.10449219 -0.19238281  0.1484375\n",
      "  0.00057983  0.296875   -0.12695312 -0.03979492  0.13183594 -0.16601562\n",
      "  0.125       0.05126953 -0.14941406  0.13671875 -0.02075195  0.34375   ]\n"
     ]
    }
   ],
   "source": [
    "print(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "f_wN9qpwA641",
    "outputId": "94e5a45e-dd88-4abf-c045-a811b9bbbd3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146951675415),\n",
       " ('berry', 0.6302295327186584),\n",
       " ('pears', 0.613396167755127),\n",
       " ('strawberry', 0.6058260798454285),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.596093475818634),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668820381165)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Воспользуемся предобученной моделью, чтобы получить синонимы для входного слова\n",
    "word2vec_model.most_similar('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель вывела в порядке убывания сходства список пар: слово – вес\n",
    "\n",
    "Давайте обучим собственную модель word2vec.\n",
    "\n",
    "Возьмем цитату с которой мы работаем. Модели word2vec нужен набор данных для обучения в виде списка токенизированных предложений. Важно: каждое предложение должно быть на отдельной строчке. Преобразуем эти предложения в нужный формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_data = [text.split() for token in text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель. Параметры в скобочках:\n",
    "\n",
    "sents_data — данные для обучения,\n",
    "size/vector_size — размер вектора,\n",
    "window — размер окна наблюдения,\n",
    "min_count — мин. частотность слова в корпусе, которое мы берем,\n",
    "sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "_EqypVmmRWDL"
   },
   "outputs": [],
   "source": [
    "custom_model = models.Word2Vec(sents_data, min_count=1,vector_size=10,workers=4, window=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=32, vector_size=10, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print (custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.save('my_v2w.model') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_w2v = gensim.models.Word2Vec.load('my_v2w.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0803047   0.00163823  0.15109275 -0.26862448  0.1581448   0.08000744\n",
      "  0.8672772   0.42532557 -0.5555587   0.08583979]\n"
     ]
    }
   ],
   "source": [
    "vector = custom_w2v.wv['я']\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('карьеру', 0.990507185459137), ('свою', 0.9896315932273865), ('более', 0.9879581332206726), ('За', 0.9851393103599548), ('9000', 0.9699773788452148)]\n"
     ]
    }
   ],
   "source": [
    "similar = custom_w2v.wv.most_similar('пропустил', topn=5)\n",
    "print (similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
